{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-04-06T15:49:39.5622303Z",
              "execution_start_time": "2023-04-06T15:47:49.1110623Z",
              "livy_statement_state": "available",
              "parent_msg_id": "f93a8831-1cd1-4873-8588-2a9973d11fdd",
              "queued_time": "2023-04-06T15:47:48.9327411Z",
              "session_id": "10",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark32large",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(spark32large, 10, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remote blob path: wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "# Azure open datasets storage information (this storage is in EastUS, ~50GB of data)\n",
        "# Leave the SAS token empty\n",
        "blob_account_name = \"azureopendatastorage\"\n",
        "blob_container_name = \"nyctlc\"\n",
        "blob_relative_path = \"yellow\"\n",
        "blob_sas_token = r\"\"\n",
        "\n",
        "# Allow SPARK to read from Blob remotely\n",
        "# If using Synapse Spark with DEP enabled workspace, this will be blocked (so use a workspace with no DEP enabled)\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "spark.conf.set(\n",
        "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
        "  blob_sas_token)\n",
        "print('Source blob path: ' + wasbs_path)\n",
        "\n",
        "# Target storage location\n",
        "# Synapse authenticates automatically using the current user to the synapse default adls storage\n",
        "# If using Databricks or other spark, use a SAS token and set it in spark conf like previous step\n",
        "adls_account_name = '<your-adls-account-name>'\n",
        "adls_container_name = '<your-container-name>'\n",
        "adls_relative_path = '<your-folder-name>'\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (adls_container_name,adls_account_name,adls_relative_path)\n",
        "print('Target blob path: ' + adls_path)\n",
        "\n",
        "# SPARK read parquet\n",
        "df = spark.read.parquet(wasbs_path)\n",
        "\n",
        "# Generate a new column by combining tpepPickupDateTime and tpepDropoffDateTime\n",
        "df1=df.withColumn(\"hashCol\", concat(date_format('tpepPickupDateTime', \"yyyyMMddhhmmss\") , date_format('tpepDropoffDateTime', \"yyyyMMddhhmmss\")))\n",
        "\n",
        "# This will copy 50GB of data from Azure open dataset source (EastUS) to your adls storage and write it as parquet with no partitions (50GB ~ 500 files ~ 100MB per file)\n",
        "df1.write.parquet(adls_path,mode='overwrite')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
