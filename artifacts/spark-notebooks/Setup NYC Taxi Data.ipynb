{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "# Azure open datasets storage information (this storage is in EastUS, ~50GB of data)\n",
        "# Leave the SAS token empty\n",
        "blob_account_name = \"azureopendatastorage\"\n",
        "blob_container_name = \"nyctlc\"\n",
        "blob_relative_path = \"yellow\"\n",
        "blob_sas_token = r\"\"\n",
        "\n",
        "# Allow SPARK to read from Blob remotely\n",
        "# If using Synapse Spark with DEP enabled workspace, this will be blocked (so use a workspace with no DEP enabled)\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "spark.conf.set(\n",
        "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
        "  blob_sas_token)\n",
        "print('Source blob path: ' + wasbs_path)\n",
        "\n",
        "# Target storage location\n",
        "# Synapse authenticates automatically using the current user to the synapse default adls storage\n",
        "# If using Databricks or other spark, use a SAS token and set it in spark conf like previous step\n",
        "adls_account_name = 'your-storage-account'\n",
        "adls_container_name = 'your-container'\n",
        "parquet_relative_path = 'nyctlc-nopartition-parquet'\n",
        "csv_gzip_relative_path = 'nyctlc-nopartition-csv-gzip'\n",
        "csv_uncompressed_relative_path = 'nyctlc-nopartition-csv'\n",
        "parquet_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (adls_container_name,adls_account_name,parquet_relative_path)\n",
        "csv_gzip_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (adls_container_name,adls_account_name,csv_gzip_relative_path)\n",
        "csv_uncompressed_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (adls_container_name,adls_account_name,csv_uncompressed_relative_path)\n",
        "print('Target parquet path: ' + parquet_adls_path)\n",
        "print('Target gzip csv path: ' + csv_gzip_adls_path)\n",
        "print('Target uncompressed csv path: ' + csv_uncompressed_adls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# SPARK read parquet\n",
        "df = spark.read.parquet(wasbs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Generate a new column by combining tpepPickupDateTime and tpepDropoffDateTime\n",
        "df_with_hashcol=df.withColumn(\"hashCol\", \n",
        "                      concat(date_format('tpepPickupDateTime', \"yyyyMMddhhmmss\") , \n",
        "                      date_format('tpepDropoffDateTime', \"yyyyMMddhhmmss\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This will write data as parquet with no partitions (78GB ~ 499 files ~ 156MB per file)\n",
        "df_with_hashcol.write.parquet(parquet_adls_path,mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This will write data as gzip compressed CSV files (61GB ~ 499 files ~ 122MB per file)\n",
        "df_with_hashcol.write.csv(csv_gzip_adls_path,mode='overwrite',compression=\"gzip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This will write data as uncompressed CSV files (261GB ~ 499 files ~ 523MB per file)\n",
        "df_with_hashcol.write.csv(csv_uncompressed_adls_path,mode='overwrite')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
