{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.window import *\r\n",
        "\r\n",
        "# Azure open datasets storage information (this storage is in EastUS, ~50GB of data)\r\n",
        "# Leave the SAS token empty\r\n",
        "blob_account_name = \"azureopendatastorage\"\r\n",
        "blob_container_name = \"nyctlc\"\r\n",
        "blob_relative_path = \"yellow\"\r\n",
        "blob_sas_token = r\"\"\r\n",
        "\r\n",
        "# Allow SPARK to read from Blob remotely\r\n",
        "# If using Synapse Spark with DEP enabled workspace, this will be blocked (so use a workspace with no DEP enabled)\r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
        "spark.conf.set(\r\n",
        "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\r\n",
        "  blob_sas_token)\r\n",
        "print('Source blob path: ' + wasbs_path)\r\n",
        "\r\n",
        "# Target storage location\r\n",
        "# Synapse authenticates automatically using the current user to the synapse default adls storage\r\n",
        "# If using Databricks or other spark, use a SAS token and set it in spark conf like previous step\r\n",
        "adls_account_name = 'vengsynapseadls001'\r\n",
        "adls_container_name = 'open-datasets'\r\n",
        "parquet_relative_path = 'nyctlc-nopartition-parquet'\r\n",
        "csv_gzip_relative_path = 'nyctlc-nopartition-csv-gzip'\r\n",
        "csv_uncompressed_relative_path = 'nyctlc-nopartition-csv'\r\n",
        "parquet_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (adls_container_name,adls_account_name,parquet_relative_path)\r\n",
        "csv_gzip_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (adls_container_name,adls_account_name,csv_gzip_relative_path)\r\n",
        "csv_uncompressed_adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (adls_container_name,adls_account_name,csv_uncompressed_relative_path)\r\n",
        "print('Target parquet path: ' + parquet_adls_path)\r\n",
        "print('Target gzip csv path: ' + csv_gzip_adls_path)\r\n",
        "print('Target uncompressed csv path: ' + csv_uncompressed_adls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# SPARK read parquet\r\n",
        "df = spark.read.parquet(wasbs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Generate a new column by combining tpepPickupDateTime and tpepDropoffDateTime\r\n",
        "df_with_hashcol=df.withColumn(\"hashCol\", \r\n",
        "                      concat(date_format('tpepPickupDateTime', \"yyyyMMddhhmmss\") , \r\n",
        "                      date_format('tpepDropoffDateTime', \"yyyyMMddhhmmss\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# This will write data as parquet with no partitions (78GB ~ 499 files ~ 156MB per file)\r\n",
        "df_with_hashcol.write.parquet(parquet_adls_path,mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# This will write data as gzip compressed CSV files (61GB ~ 499 files ~ 122MB per file)\r\n",
        "df_with_hashcol.write.csv(csv_gzip_adls_path,mode='overwrite',compression=\"gzip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# This will write data as uncompressed CSV files (261GB ~ 499 files ~ 523MB per file)\r\n",
        "df_with_hashcol.write.csv(csv_uncompressed_adls_path,mode='overwrite')"
      ]
    }
  ]
}