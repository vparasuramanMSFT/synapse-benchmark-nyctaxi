# Performance Benchmarking of Azure Synapse Analytics using NYC Taxi Open dataset

This repository provides sample data and sample scripts that can be used to perform various benchmarking tests for Azure Synapse Analytics.
Initial release includes benchmarking for Dedicated SQL Pools. Other services will be added in the future releases.

## Prerequisite
In order to run this performance benchmarking, you would need the following resources created in Azure:
- Azure Synapse Analytics Workspace
- Dedicated SQL Pool inside the workspace (SKU: DW1000c)
- Apache Spark Pool inside the workspace (optional)
- Azure Storage Account (You can use the default storage account of Azure Synapse Analytics workspace)

Please Note: 
1. It is recommended to use DWU1000c (or above) for the dedicated SQL Pools to get optimal results for the performance benchmarking.
Reference: [Azure Synapse proof of concept playbook](https://learn.microsoft.com/en-us/azure/synapse-analytics/guidance/proof-of-concept-playbook-dedicated-sql-pool#setup)

2. We are going to use Synapse Spark to setup the dataset, so you would need an Apache Spark Pool to attach and run our sample notebook. But if you plan to use other spark providers (such as Databricks), you may need not create Apache Spark pool inside the Synapse workspace.

3. If you are going to use Synapse Spark to setup the dataset, you may need to create your Synapse workspace without [Data Exfiltration Protection](https://learn.microsoft.com/en-us/azure/synapse-analytics/security/workspace-data-exfiltration-protection). Enabling DEP will block the download of the sample dataset. Enabling [Managed VNets](https://learn.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-vnet) should be fine.

4. Our tests are conducted based on having the Azure Storage Account and Synapse Workspace in the same Azure region. This will impact the copy performance.

## About the dataset
We are going to use NYC Taxi yellow dataset for our performance benchmarking. This dataset is provided by Microsoft as part of [Azure Open datasets](https://learn.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets)

This dataset is stored in Parquet format. There are about 1.5B rows (50 GB) in total. This dataset contains historical records accumulated from 2009 to 2018. This dataset is stored in the East US Azure region.

You can learn more about the structure and sample of the dataset from this link: [NYC Taxi & Limousine Commission - yellow taxi trip records](https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=azureml-opendatasets)

## Setting up the dataset
Before running the performance tests, you have to download (copy) the NYC Taxi dataset to your Azure Storage Account. The source storage account is in East US Azure region. Your target storage account can be located anywhere (ideally it should be located in the same region as your Synapse workspace). 

For our exercise, we are modifying the source dataset by adding one additional columns to hash distribute our tables called 'hashCol'.
The 'hashCol' is generated by concatenating the tpepPickupDateTime and tpepDropoffDateTime columns.

You can use the [Copy NYC Taxi Data](artifacts/spark-notebooks/Copy%20NYC%20Taxi%20Data.ipynb) notebook to download the data and generate the modified dataset with our additional column. The code is written using pyspark, so you would need a spark environment to run your code.

## Data loading with single user
In this exercise, we will load 1.5B records (50GB) of data from parquet files to dedicated SQL pool. As stated in the prerequisites, we will be running a DW1000c for our test. You can use the SQL script provided here: [Data loading with single user](artifacts/sql-scripts/Data%20loading%20with%20single%20user.sql)

### Results

| Command | Description | Resource Class | Distribution | Index | Average Duration |
| --- | --- | --- | --- | --- | --- |
| COPY INTO [your-table] from [your-parquet-files] | Copies the data from external parquet files stored in Azure Storage to your table in dedicated SQL Pool | smallrc | Round Robin | Heap | 13 mins |
| COPY INTO [your-table] from [your-parquet-files] | Copies the data from external parquet files stored in Azure Storage to your table in dedicated SQL Pool | smallrc | Hash | Heap | 19 mins |
| CTAS into [your-cci-table] | Copies the data from round robin heap table to hash CCI table, redistributing the data | largerc | Hash | CCI | 9 mins |
| CTAS into [your-cci-table] | Copies the data from hash heap table to hash CCI table without redistributing the data | largerc | Hash | CCI | 5 mins |

 
