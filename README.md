# Performance Benchmarking of Azure Synapse Analytics using NYC Taxi Open dataset

This repository provides sample data and sample scripts that can be used to perform various benchmarking tests for Azure Synapse Analytics.
Initial release includes benchmarking for Dedicated SQL Pools. Other services will be added in the future releases.

## Prerequisite
In order to run this performance benchmarking, you would need the following resources created in Azure:
- Azure Synapse Analytics Workspace
- Dedicated SQL Pool inside the workspace (SKU: DW1000c)
- Apache Spark Pool inside the workspace (optional)
- Azure Storage Account (You can use the default storage account of Azure Synapse Analytics workspace)

Please Note: 
1. It is recommended to use DWU1000c (or above) for the dedicated SQL Pools to get optimal results for the performance benchmarking.
Reference: [Azure Synapse proof of concept playbook](https://learn.microsoft.com/en-us/azure/synapse-analytics/guidance/proof-of-concept-playbook-dedicated-sql-pool#setup)

2. We are going to use Synapse Spark to setup the dataset, so you would need an Apache Spark Pool to attach and run our sample notebook. But if you plan to use other spark providers (such as Databricks), you may need not create Apache Spark pool inside the Synapse workspace.

3. If you are going to use Synapse Spark to setup the dataset, you may need to create your Synapse workspace without [Data Exfiltration Protection](https://learn.microsoft.com/en-us/azure/synapse-analytics/security/workspace-data-exfiltration-protection). Enabling DEP will block the download of the sample dataset. Enabling [Managed VNets](https://learn.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-vnet) should be fine.

4. Our tests are conducted based on having the Azure Storage Account and Synapse Workspace in the same Azure region. This will impact the copy performance.

## About the dataset
We are going to use NYC Taxi yellow dataset for our performance benchmarking. This dataset is provided by Microsoft as part of [Azure Open datasets](https://learn.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets)

This dataset is stored in Parquet format. There are about 1.5B rows (50 GB) in total. This dataset contains historical records accumulated from 2009 to 2018. This dataset is stored in the East US Azure region.

You can learn more about the structure and sample of the dataset from this link: [NYC Taxi & Limousine Commission - yellow taxi trip records](https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=azureml-opendatasets)

## Setting up the dataset
Before running the performance tests, you have to download (copy) the NYC Taxi dataset to your Azure Storage Account. The source storage account is in East US Azure region. Your target storage account can be located anywhere (ideally it should be located in the same region as your Synapse workspace). 

For our exercise, we are modifying the source dataset by adding one additional columns to hash distribute our tables called 'hashCol'.
The 'hashCol' is generated by concatenating the tpepPickupDateTime and tpepDropoffDateTime columns.

Also, we will be writing the data out in three different formats for comparison:
- Parquet (78GB ~ 499 files ~ 156MB per file)
- CSV GZip (61GB ~ 499 files ~ 122MB per file)
- CSV (uncompressed) (261GB ~ 499 files ~ 523MB per file)

You can use the [Setup NYC Taxi Data](artifacts/spark-notebooks/Setup%20NYC%20Taxi%20Data.ipynb) notebook to download the data and generate the modified dataset with our additional column. The code is written using pyspark, so you would need a spark environment to run your code.

## Data loading with single user
In this exercise, we will load 1.5B records of data from parquet/csv gzip/csv files to dedicated SQL pool. As stated in the prerequisites, we will be running a DW1000c for our test. You can use the SQL scripts provided here: [sql-scripts](artifacts/sql-scripts/)

### Results

| Command | Source Format | Total Size | Resource Class | Distribution | Index | Average Duration | Bytes Processed (GB) | Avg Mbps |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| COPY INTO | CSV GZip | 61 GB | smallrc | Round Robin | Heap | 11 mins | 216 | 314 |
| COPY INTO | CSV GZip | 61 GB | smallrc | Hash | Heap | 12 mins | 216 | 292 |
| COPY INTO | CSV | 261 GB | smallrc | Round Robin | Heap | 12 mins | 216 | 290 |
| COPY INTO | Parquet | 78 GB | smallrc | Round Robin | Heap | 14 mins | 264 | 320 |
| COPY INTO | CSV | 261 GB | smallrc | Hash | Heap | 14 mins | 216 | 261 |
| COPY INTO | Parquet | 78 GB | smallrc | Hash | Heap | 15 mins | 264 | 295 |
| COPY INTO | CSV | 261 GB | smallrc | Round Robin | CCI | 18 mins | 216 | 197 |
| COPY INTO | CSV GZip | 61 GB | smallrc | Round Robin | CCI | 19 mins | 216 | 190 |
| COPY INTO | Parquet | 78 GB | smallrc | Round Robin | CCI | 19 mins | 264 | 231 |
| COPY INTO | CSV | 261 GB | smallrc | Hash | CCI | 20 mins | 216 | 177 |
| COPY INTO | CSV GZip | 61 GB | smallrc | Hash | CCI | 24 mins | 216 | 152 |
| COPY INTO | Parquet | 78 GB | smallrc | Hash | CCI | 26 mins | 264 | 172 |

As we can infer from the above table:
> Fastest load is achieved using: **CSV Gzip - Round Robin - Heap (11 mins)**

> Slowest load was using: **Parquet - Round Robin - CCI (26 mins)**

In terms of formats:

> **CSV GZip < CSV < Parquet**

In terms of distribution: Round Robin is faster than Hash:

> **Round Robin < Hash**

In terms of indexing:

> **Heap < CCI**

So in combination, for any given format:

> **Round Robin Heap < Hash Heap < Round Robin CCI < Hash CCI**

But if you have to get an optimal performance in loading to final table with CCI, it might be worth considering ***Hash Heap*** while loading the staging, as it will subsequently reduce the data movements operations for further CTAS in your ETL. Considering the time difference between GZip Round Robin and GZip Hash, it is 11 min vs 12 min, but that will offset the subsequent CTAS load performance by avoiding some data movements.